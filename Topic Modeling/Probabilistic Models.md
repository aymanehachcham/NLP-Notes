# What is it
* A generative model with a set of parameters that will help define the probabilities of all the observed data points. 
* Since we want to model $k$ number of topics $\{\theta_{1},...,\theta_{k}\}$, with a probability word distribution $P(w|\theta_{j}), \,  w \in V$ and a topic coverage distribution $\pi_{ij}$ for all the documents $d_{1},...,d_{i},...,d_{N}$ in the collection $C$. Therefore, our model will have $kN$ parameters for topic coverage and $k|V|$ for word distribution.
* We need to infer the model parameters $kN + k|V|$ that will maximise the probabilities of the documents and words under the model.

![image](pics/proba_model.png)

## Unigram Language Model
* In the case of a Unigram Language Model, we have one document $d_{i}$ and we want to extract one topic $\theta_{i}$.
* We assume that all the words inside the document $d_{i}$ were generated by a probabilistic model and we assign a probability to each word: $P(w_{i}|d)$.
* For $M$ words in the document $d_{i}$, we define the probability distribution of the words $P(w_{i}|d)$ as the likelihood: $$w \in V, \,\, P(w_{i}|d) \,\,=\,\, P(w_{1}|d)\cdot P(w_{2}|d) \cdot P(w_{3}|d)...P(w_{m}|d)\,\,=\,\,\prod_{i=1}^{M}P(w_{i}|d)$$
* We need to add the count of each word in the document $c(w_{i},d)$ to the probability of the word: $P(w_{i}|d)^{c(w_{i},d)}$. Therefore, our likelihood function becomes: $$\prod_{i=1}^{M}P(w_{i}|d)^{c(w_{i},d)}$$
### Maximum Likelihood Estimate
* The probability of each word $P(w_{i}|d)$ can be expressed as $\theta_{i}$. We need to find the best estimate for $\theta_{i}$ that will maximise the probability of observing the data.
* Under the constraint that: $\sum_{i=1}^{M} \theta_{i} = 1$.
* We use a Lagrangian approach on the Log-Likelihood: $$\begin{align}Log-Likelihood = \sum_{i=1}^{M} \,\, c(w_{i},d) \cdot log(\theta_{i}) \\[.2cm] Langrange- Multiplier = \lambda \left( \sum_{i=1}^{M} \theta_{i} - 1 \right) \\[.2cm]  Therefore, \,\,\,  L(w|d) = \sum_{i=1}^{M} \,\, c(w_{i},d) \cdot log(\theta_{i}) \,\,\, + \lambda \left( \sum_{i=1}^{M} \theta_{i} - 1 \right)\end{align}$$
* We need to derivate and equal to zero this last part: $\sum_{i=1}^{M} \,\, c(w_{i},d) \cdot log(\theta_{i}) \,\,\, + \lambda \left( \sum_{i=1}^{M} \theta_{i} - 1 \right)$
* We therefore have the following: $$\frac{\partial L(w|d)}{\partial \theta_{i}} = 0 \rightarrow \sum_{i=1}^{M} \,\, \frac{c(w_{i},d)}{\theta_{i}} \,\, + \lambda$$
* Finally, the best estimate for the model is: $\hat{\theta_{i}} = \frac{c(w_{i}|d)}{|d|}$. The best estimate is the frequency of the words. The probability is given as the relative frequency if each word in the document. 

## The Probabilistic LSA:
* A probabilistic version of the LSA algorithm:
![image](images/plsa.png)
* The probability of a word given a document is the product of the following quantities:
	* Probability of word give document: $P(w | d)$
	* The prior probability of each topic: $P(k)$
	* The probability of document given a topic: $P(d|k)$
$$ P(w|d) = \sum_{k=1}^{K} P(k)\cdot P(w|d) \cdot P(d|k) = P(d)\sum_{k=1}^{k} P(w|k)\cdot P(k|d) $$
* The estimation of the PLSA algorithm is done using EM Algorithm:
	* **Step 1**: Initialize the parameters
	* **Step 2**: Calculate the expected latent variables from model params and data
	* **Step 3:** Update the model params to maximise the likelyhood of observing the data

#### An idea about Mixed membership models
![image](images/mmber.png)

## LDA: Latent Dirichlet Allocation
* Used to detect underlying topics in text documents
#### Assumptions of LDA:
* Documents with similar topics will use similar group of words
* Documents are proba distributions over latent topics
* Topics are proba distributions over words
![image](images/lda.png)

#### Plate Notaion
![image](images/lda_plate.png)

* Rectangle M: total number of documents in the corpus
* Rectangle N: total number of words in a document
* $\alpha, \beta$: Dirichlet priors 
	* $\alpha$: Dirichlet prior on the per document topic distribution -> high $\alpha$ each document high mixture of topics, low $\alpha$ only few topics.
	* $\beta$: Dirichlet prior on the per topic word distribution -> high $\beta$ each topic high mixture of words, low $\beta$ only few words.
* $\theta$: Multinomial distribution for topics according to prior $\alpha$.
* $\phi$: Bunch of Multinomial distributions for the words according to prior $\beta$.
* T: list of topics
* W: combine list of topics $T$ and Multinomial word distributions to get the document d.
* Repeat to get the whole corpus M.

### Generative Process of LDA
**The Goal is to generate documents with a words-topic distribution**
* Pick a number of words per document: $n$
* Initialize the Dirichlet document-topic distribution for a document $d$, prior $\alpha$: $$ \prod_{j=1}^{M} P(\theta_{j}\,|\,\alpha) $$

* Build the Multinomial distribution of $n$ words according to the topic distribution in document $d$: $$ \prod_{i=1}^{n} P(Z_{j,i} \,|\,\theta_{j}) $$
* Initilialize the Dirichlet topic-word distribution, prior $\beta$: $$ \prod_{t=1}^{K} P(\psi_{t} \,|\,\beta) $$
* Build the multinomial distribution of $n$ words according to the word distribution in prior $\beta$: $$ P(W_{j,i} \,|\,\psi_{Z_{i,t}}) $$
![image](images/tops.png)


### Problems with LDA models
* LDA is a soft clustering
* Depends heaviliy on the choice of the priors
* Estimation via Gibbs sampling is stochastic
* In practice it leads to strongly different topics

